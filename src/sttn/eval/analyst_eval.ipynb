{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "##                                                                                                   ##\n",
    "##    VARIABLES \"dataset_name\", \"exp_prefix\", \"exp_version\", \"model_name\", \"code_retry_limit\" AND    ##\n",
    "##           \"max_concurency\" WILL BE CREATED BY THE nbconvert.ExecutePreprocessor ABOVE             ##\n",
    "##                                                                                                   ##\n",
    "#######################################################################################################\n",
    "# dataset_name='Taxi + LEHD evaluation - 280 examples'\n",
    "# exp_prefix='baseline-29-01_'\n",
    "# exp_version='1.0.0'\n",
    "# model_name='gpt-4o-mini'\n",
    "# code_retry_limit=2\n",
    "# max_concurrency=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sttn.nli.analyst import STTNAnalyst\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, traceable\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "import backoff\n",
    "import openai\n",
    "import traceback\n",
    "import gc\n",
    "import re\n",
    "import numpy as np\n",
    "import sttn.algorithms.community.detection\n",
    "\n",
    "from sttn.eval.evaluators import (Evaluators, SummaryEvaluators, max_tries,\n",
    "                                                       base,\n",
    "                                                       factor,\n",
    "                                                       max_value)\n",
    "\n",
    "set_debug(False)\n",
    "\n",
    "# Langsmith client\n",
    "client = Client()\n",
    "\n",
    "# Evaluating LLM\n",
    "eval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o\", max_retries=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload `langsmith dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "print(f\"\\n\\n------------Info about {test_dataset.name} dataset------------\\n\")\n",
    "for key, value in test_dataset.dict().items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap `STTNAnalyst.chat` results in a compatible function with backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, (openai.RateLimitError), max_tries=max_tries, base=base, factor=factor, max_value=max_value)\n",
    "def get_context_with_backoff(inputs: dict, model_name: str, code_retry_limit: int):\n",
    "    analyst = STTNAnalyst(model_name=model_name, code_retry_limit=code_retry_limit)\n",
    "    #print(f\"\\nQuery_ID: {inputs['id']}, DEBUG:\\n\\tLaunched analyst...\")\n",
    "    context = analyst.chat(user_query=inputs[\"question\"])\n",
    "    context.analysis_code = str(context.analysis_code) if context.analysis_code else ''\n",
    "    #print(f\"Query_ID: {inputs['id']}, DEBUG:\\n\\tContext returned!\")\n",
    "    return context\n",
    "\n",
    "def delete_generated_temp_vars(analysis_code: str, id: int):\n",
    "    temp_vars = set(re.findall(r'\\b([a-zA-Z_]\\w*)\\b\\s?=(?!=)', analysis_code))\n",
    "    for var in temp_vars:\n",
    "        try:\n",
    "            # Dereference objects\n",
    "            globals()[var] = None\n",
    "            del globals()[var]\n",
    "            #print(f\"\\nQuery_ID: {id}, DEBUG:\\n\\tDeleted variable `{var}`\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(f\"\\nQuery_ID: {id}, WARNING:\\n\\tCan't delete variable `{var}`\\n\\tError message: {e}\\n\")\n",
    "    #gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def analyst_results(inputs: dict, model_name=model_name, code_retry_limit=code_retry_limit) -> dict:\n",
    "    \"\"\"\n",
    "    Wrapper function to get the results from the Analyst and return them in a dictionary\n",
    "    Args:\n",
    "        inputs: dict, the inputs to the Analyst\n",
    "        model_name: str, the name of the model to use\n",
    "        code_retry_limit: int, the number of times to retry the code\n",
    "    Returns:\n",
    "        dict, the results from the Analyst\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty result dictionary\n",
    "    empty_result_dict = {\"data_provider_id\": \"\",\n",
    "                         \"data_provider_args\": {},\n",
    "                         \"result\": None,\n",
    "                         \"executable\": False,\n",
    "                         \"analysis_code\": \"NO CODE FROM ANALYST, RETURN 0\"}\n",
    "    \n",
    "    print(f\"\\nQuery_ID: {inputs['id']}, INFO:\\n\\tQuery:', {inputs['question']}\\n\")\n",
    "    # Get the context from the Analyst\n",
    "    try:\n",
    "        context = get_context_with_backoff(inputs=inputs, model_name=model_name, code_retry_limit=code_retry_limit)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nQuery_ID: {inputs['id']}, ERROR:\\n\\tAn error happened while launching Analyst (return empty dict instead)\\n\\tError message:\")\n",
    "        traceback.print_exc()\n",
    "        print(\"\\n\\t|| END OF ERROR ||\\n\\n\")\n",
    "        return empty_result_dict\n",
    "    \n",
    "    try:\n",
    "        #print(f\"\\nQuery_ID: {inputs['id']}, DEBUG:\\n\\tChecking context for query ID {inputs['id']}:\")\n",
    "        # Get the data provider from the context if it exists\n",
    "        if not context.data_provider or not context.feasible:\n",
    "            return empty_result_dict\n",
    "        else:\n",
    "            #print(f'\\tData provider: {context.data_provider}')\n",
    "            data_provider = context.data_provider\n",
    "\n",
    "        # Get the data provider id and args from the context if they both exist\n",
    "        if context.data_provider_id and context.data_provider_args:\n",
    "            data_provider_id = context.data_provider_id\n",
    "            # lowercase the values of context.data_provider_args dictionary\n",
    "            data_provider_args = context.data_provider_args\n",
    "            data_provider_args = {k: v.lower().strip() if isinstance(v, str) else v for k, v in data_provider_args.items()}\n",
    "            #print(f\"\\tData provider ID: {data_provider_id}\\n\\tData provider args: {data_provider_args}\")\n",
    "        else:\n",
    "            return empty_result_dict\n",
    "        \n",
    "        # Turn the result into a float if possible else None\n",
    "        try:\n",
    "            #print(f\"\\tResult: {context.result}\\n\")\n",
    "            context.result = float(context.result)\n",
    "            result = round(context.result, 5)\n",
    "            if np.isnan(result):\n",
    "                result = None\n",
    "        except:\n",
    "            print(f\"\\nQuery_ID: {inputs['id']}, WARNING:\\n\\tError while converting result to float (return None instead)\\n\\tError message:{e}\\n\\t|| END OF ERROR ||\\n\")\n",
    "            result = None\n",
    "        \n",
    "        # Add an evaluation query to the analysis code for the geospatial/temporal awareness\n",
    "        if context.analysis_code:\n",
    "            analysis_code = f\"We have the following data structure: {data_provider.__doc__} \\\n",
    "                            \\n{data_provider.get_data.__doc__}\\\n",
    "                            \\nWe retrieved the SpatioTemporalNetwork with the following arguments {context.data_provider_args}\\\n",
    "                            \\nThe code looks like this:\\n\" + str(context.analysis_code) \n",
    "        else:\n",
    "            analysis_code= \"NO CODE FROM ANALYST, RETURN 0\"\n",
    "        \n",
    "        # extract temporary names from the analysis code and delete them\n",
    "        #print(f\"Query_ID: {inputs['id']}, DEBUG:\\n\\tDelete temp vars...\")\n",
    "        delete_generated_temp_vars(context.analysis_code, inputs['id'])\n",
    "\n",
    "        output = {\"data_provider_id\": data_provider_id,\n",
    "                \"data_provider_args\": data_provider_args,\n",
    "                \"result\": result,\n",
    "                \"executable\": True,\n",
    "                \"analysis_code\": analysis_code,\n",
    "                }\n",
    "        \n",
    "        # print(f\"Query_ID: {inputs['id']}, DEBUG:\\n\\tFinal output:\", output)\n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nQuery_ID: {inputs['id']}, ERROR:\\n\\tUnexpected error appeared while transforming Analyst's output\\n\\tError message:{e}\\n\\t|| END OF ERROR ||\\n\")\n",
    "        return empty_result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = Evaluators(eval_llm)\n",
    "summary_evaluators = SummaryEvaluators(evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst = STTNAnalyst(model_name=model_name, verbose=False)  # model_name is initialized by runner script at the begginning of the notebook\n",
    "\n",
    "chain_results = evaluate(\n",
    "    analyst_results,  #  AI system (wrapped function with outputs as dict),\n",
    "    data=test_dataset.name,  # The dataset name to predict and grade over\n",
    "    evaluators=[\n",
    "        evaluators.data_provider_id_match,\n",
    "        evaluators.data_provider_args_match,\n",
    "        evaluators.executable_match,\n",
    "        evaluators.result_match,\n",
    "        evaluators.get_geosp_aware_eval_score,\n",
    "        evaluators.get_temp_aware_eval_score,\n",
    "        ],  # The evaluators to score the results\n",
    "    summary_evaluators=[\n",
    "        summary_evaluators.taxi_dp_id_accuracy_summary_eval,\n",
    "        summary_evaluators.taxi_dp_args_accuracy_summary_eval,\n",
    "        summary_evaluators.taxi_dp_result_accuracy_summary_eval,\n",
    "        summary_evaluators.lehd_dp_accuracy_summary_eval,\n",
    "        summary_evaluators.lehd_dp_args_accuracy_summary_eval,\n",
    "        summary_evaluators.lehd_dp_result_accuracy_summary_eval,\n",
    "        summary_evaluators.geospatial_awr_result_accuracy_summary_eval,\n",
    "        summary_evaluators.temporal_awr_result_accuracy_summary_eval,\n",
    "        summary_evaluators.comm_det_result_accuracy_summary_eval,\n",
    "        summary_evaluators.pagerank_result_accuracy_summary_eval,\n",
    "        summary_evaluators.net_dens_result_accuracy_summary_eval,\n",
    "        summary_evaluators.cen_deg_result_accuracy_summary_eval,\n",
    "        summary_evaluators.clust_coef_result_accuracy_summary_eval,\n",
    "        summary_evaluators.poorly_written_args_accuracy_summary_eval,\n",
    "        summary_evaluators.poorly_written_result_accuracy_summary_eval,\n",
    "        ],  # summary evluators to score the overall results\n",
    "    experiment_prefix=exp_prefix,  # A prefix for your experiment names to easily identify them\n",
    "    metadata={\n",
    "      \"version\": f\"{exp_version}\",\n",
    "    },\n",
    "    max_concurrency=max_concurrency,  # The maximum number of concurrent evaluations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n------------------Evaluation metrics------------------\")\n",
    "results_dict = {}\n",
    "counts_dict = {}\n",
    "try:\n",
    "    for example_res in chain_results._results:\n",
    "        for eval_metric in example_res['evaluation_results']['results']:\n",
    "            if eval_metric.key == '__ignore__': continue\n",
    "            elif eval_metric.key in results_dict:\n",
    "                results_dict[eval_metric.key] += eval_metric.score\n",
    "                counts_dict[eval_metric.key] += 1\n",
    "            else:\n",
    "                results_dict[eval_metric.key] = eval_metric.score\n",
    "                counts_dict[eval_metric.key] = 1\n",
    "\n",
    "\n",
    "    # Fancy print\n",
    "    for metric, score in results_dict.items():\n",
    "        if score >= 0:\n",
    "            print(f\"'{metric}': {score/counts_dict[metric]}\")\n",
    "    print('\\n')\n",
    "    for sum_eval_metric in chain_results._summary_results['results']:\n",
    "        results_dict[sum_eval_metric.key] = sum_eval_metric.score\n",
    "        if sum_eval_metric.score >= 0:\n",
    "            print(f\"'{sum_eval_metric.key}': {sum_eval_metric.score}\")\n",
    "\n",
    "    print('\\nEmpty/ignored metrics below:')\n",
    "    for metric, score in results_dict.items():\n",
    "        if metric.startswith('__ignore'):\n",
    "            continue\n",
    "        elif score < 0:\n",
    "            print(f\"'{metric}': {score}\")\n",
    "\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'\\nERROR:\\n\\tAn error occurred while processing the results: {str(e)}')\n",
    "    print(\"Please check the results by the link below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
